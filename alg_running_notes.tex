\documentclass{article}


\title{Algorithms Notes}
\author{Karl Pichotta}
\date{Spring 2013}

\usepackage{amsmath,amssymb}


\newcommand{\inv}{^{-1}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\begin{document}

\maketitle

\begin{abstract}
These are  notes I've been taking for a graduate algorithms
course taught by Greg Plaxton at UT Austin.
They're taken primarily in-class, and probably only useful for my own
 personal reference.
\end{abstract}

\section{Preamble: Useful identities}




\begin{itemize}

	\item For nonzero $a,b,c$, we have
	\beq
	a^{\log_b c} = 
	c^{\log_b a}
	\eeq

	\item
	For large $n$, we have
	\begin{align}
		\left(
			1 + \frac{1}{n}
		\right)
		&\approx 
		e
		\\
		\left(
			1 - \frac{1}{n}
		\right)
		&\approx 
		e\inv
	\end{align}
	
	\item For nonzero integer $n$, we have
	\beq
	n^2 = n + 2{n\choose 2}.
	\eeq
	Both the algebraic and intuitive justifications for this identity
	should be pretty clear.

\end{itemize}

\section{1/22/2013: Chernoff Bounds, the Tails of the Binomial}

If we have a  binomaial RV $X\sim B(n,p)$, then we have the following
 Chernoff bound, for $0\leq \delta \leq 1$:
$$
P(X\leq (1-\delta) np) \leq
\exp\left\{
	\frac{-\delta^2 np} {2}
\right\}
$$

Supposing we throw $100 n\log n$ balls, by focusing our analysis on a single bin
and then applying Union Bound, we get that the probability that 
some $X_i \leq \log n$ is $\leq n^{-44}$, using the above bound.

\textbf{Fact:}
Thinking about Hashing, suppose we throw $n$ balls into $n$ bins, and we want a bound on the max load of any given bin.
We might hope it's $O(1)$, but it isn't.
Instead, it is
$$
\Theta\left(
	\frac{\log n} {\log \log n}
\right)
$$
with high probability.

\textbf{Upper bound:}
Define $Z = \max_i X_i$, with $X_i$ the number of balls in bin $i$.
Consider bin $1$.
$E[X_1] = 1$, but this tells us nothing about the tail of the distribution,
which is our interest here.

What then, is
$$
Pr\left\{
	X_1 \geq c\frac{\log n}{ \log\log n}
\right\}
$$
Framing it in terms of Chernoff bounds, we will want to use:
$$
\delta = c\frac{\log n}{ \log\log n} - 1 
%\approx
%c\frac{\log n}{ \log\log n} 
$$
We need to use the Large Deviations bound (bound (2) on handout).
This is the following:
Suppose $X\sim Binom(n,p)$, then
$$
Pr\left\{
	X\geq (1 + \delta) np
\right\}
\leq
\left(
	\frac{e^\delta}{(1+\delta)^{1+\delta}}
\right)
^{np}
$$
For concreteness, we take $c=100$.
Our $np=1$, so the RHS of the above chernoff bound is
$$
... \leq
\left(
\frac{e^{\delta + 1}}
{	\left(100 \frac{\log n}{\log \log n} \right) ^ {100 \log n / (\log \log n)}}
\right) 
\leq
\left(
	\frac{\log\log n}{\log n}
\right) ^ {100 \log n / (\log \log n)}
\approx
n^{-100}
$$
WE inflate the numerator by adding an extra $e$.

Sidenote:
Now, this funny $\log\log $n terms come from the fact that solving
$$
x^x = n
$$
gives you something very like $\log n / \log\log n$.
This comes from canceling out lower order approximations to log.

Returning, by union bound, we argue that the probability that some bin gets $\geq 100 \log n / \log\log n$ balls is $\leq n^{-99}$.

\textbf{Lower bound:}
So that was an upper bound. The lower bound is trickier.
Let $k = \varepsilon \log n / \log\log n$, with $\varepsilon$ a small positive argument.
We won't be able to just reason about bin 1 and then use union bound---there is a constant probability $(1 - 1/n)^n \approx e^{-1}$ that bin 1 gets 0 bins.
Let $E_i$ denote the event that bin $i$ receives at least $k$ balls.
We want to show that
$$
Pr(\cup E_i) \geq 1 - \frac{1}{n^c}
$$
This is, reminder, a lower bound on the max.
That is, we're showing that
$$
Z = \Omega(\log n / \log\log n).
$$
We're showing that $Pr(E_1)$ is small, but it'll be useful.

So consider $Pr(E_1)$.
We have
$$
Pr(E_1) \geq Pr(X_1 = k)
$$
(since RHS entails LHS)
This is equal to 
\begin{align}
Pr(X_1 = k)
&= {n \choose k} 
\left(1/n \right)^k
(1 - 1/m)^{n-k}
\\
&\geq
{n\choose k} (1/n)^k
(1/e)
\\
&\geq \left(
	\frac{n}{k}
\right)^k
(1/n)^k
(1/e)
\\
&=
\frac{1}{e k^k}
\\
&\approx \frac{1}{n^{\varepsilon'}}
\end{align}
We have the third to last step by the following useful lower bound:
$$
{n\choose k} \geq (n/k)^k
$$
which you see by expanding the formula and reasoning about the various values of quantities above and below the line.

Now, by linearity of expectation, we expect
$n^{1-{\varepsilon'}}$ bins to get at least $k$ balls.

What we want, however, is a high probability bound.
That is, we want a statement of the form ``at least one of  bins, whp, will get at least $k$ bounds''.
We can't use a Chernoff bound, because the $E_i$'s are not independent, so therefore the distribution isn't Binomial.

From a highl-level perspective: we throw $n$ balls into $n$ bins, then ask if $E_1$ occurred.
If it did, then we'll be happy, because all we want is one of the $E_i$s to occur (NB $PR(E_1) \geq 1 / (n^{\varepsilon'}$).
If $E_1$ doesn't occur, then note
$$
PR(E_2 | E_1^c) \geq Pr(E_2) \geq 1 / {n^{\varepsilon'}}
$$
and so on with all of the $Pr(E_i)$'s.
We can therefore argue that the probbability that no$ E_i$ occurs is
$$
\leq (1 - 1/n^{\varepsilon'})^n
$$
We want to factor this last equation into
\begin{align}
...&=
((1 - 1/n^{\varepsilon'}) ^ {n^{\varepsilon'}} ) ^ {n^{1 - \varepsilon'}}
\\
&\approx
(1/e) n^{1 - \varepsilon'}
\end{align}
Instead of having $1/e$ to some logarithmic quantity, we have it to some polynomial quantity.
So this is much, much less than inverse polynomial bound, and we've more than satisfied our sharp threshold.

An exercise close to the one we did at the begnning---using Chernoff bounds,
we can argue that the number of flips of a fair coin to get $\log n$ heads
with high probability is
$$
\Theta(\log n)
$$
(we use Chernoff bound (1), as in the first example).
This comes up in at least one way to analyze randomized Quicksort.

In last class, we saw that the expected number of comparisons is $\Theta(n\log n)$.
We now argue that, with high probability, the number of comparisons is $O(n\log n)$.
Note this is quite different: what we say now is that the probability that you 
exceed the expected runtime by a factor of, say, 50, is very small.
Note these RVs aren't binomial, but we'll be able to bound their behavior
with binomial RVs.

First, we'd LIKE to show the number of comparisons involving a specific key is $O(\log n)$ with 
high probability.
This is not true---there is a $1/n$ chance of an element being the first
pivot, and it gets compared to $n$ elements.

So what we'll do instead is to use a charging scheme: we charge
a comparison to a non-pivot.
Whenever we make a comparison, ``charge'' the comparison to the nonpivot (each comparison involves a pivot and a nonpivot).
It IS true that the charge to any key is $O(\log n)$ with high probability.
Once we show that, it immediately follows that the total number of comparisons is $O(n\log n)$ WHP.
HOw can we use the prvious result to convince ourselves of this?



\section{1/24/2013: Hashing}

Let's think about Randomized quicksort.
We know that randomized quicksort has expected $O(n\log n)$ behavior.
(that is, there are no ``bad inputs'' for it in expectation).
Note also that Randomized quicksort has $O(n\log n)$ runtime with high
probability (that is, we'll give a nice bound on the probability of 
the runtime being asymptotically higher).


We can think about quicksort from the perspective of a particular fixed
key, call it $x$.
Recall the charging scheme: we ``charge'' each comparison to the nonpivot 
(which we can do, since each comparison is between a pivot and another value).

\textbf{Claim:}
The key $x$ gets $O(\log n)$ charge with high probability (the failure
probability will be $1/n^c$ for arbitrarily large $c$.
(If we prove this, then by Union Bound, we have the total charge to all
$n$ keys is $O(n\log n)$ WHP.)

\textbf{Proof of Claim:}
We have a ``herd'' of keys, initially of size $n$, and select one to be the pivot.
The pivot is remarkably lucky---it gets 0 charge.
If $x$ is chosen as the pivot, that's nice.
If, on the other hand, $x$ is not chosen as pivot, then it will get charge 1.
Then the keys will be partitioned into two ``herds'', one of which will
never again be compared with $x$.

Consider the process where we start with a positive integer $n$.
We flip a coin. 
If we get heads, we set the number to an integer uniformly drawn from
$[0, 3n/4]$ (floored).
If we get tails, then we set the value uniformly to between $0$ and $n - 1$.
So at each stage, the value gets smaller (by at least one).
The process stops when the number gets to 0.

We use this process as follows. 
Picking a pivot at random,
there is a $0.5$ probability that the pivot sits in between $n/4$ and $3n/4$ in the sorted list (it sits in the middle half).
There's also a $.5$ chance it's not in the middle.
The number in the process is just the size of the ``herd'' $x$ is in.

If we get a good pivot, then the herd value is at most $3n/4$ (imagine
getting the rightmost pivot in the middle half, say).
This is like flipping a head.
In the worst case (flipping a tail), then the size of the herd decreases by 1.

The process can't involve more than some constant times $\log n$ heads:
each head decreases the herd size by at least $3/4$.
(Prove this?)
%The maximum number of heads needed to get to 1 is $i$ in
%$$
%\left(
%\frac{3}{4}
%\right)
%^i n = 1
%$$
So $x$ sits there and hopes for good pivot choices from its perspective.
(This is basically the end of the proof.)

We don't have a binomial variable, but we can relate it to this
simpler process defined above, and use a Chernoff bound on that.


\subsection{Hashing with chained overflows}

We have a hashtable, visualized as an array of buckets, numbered $0,\ldots,k-1$.
We have a has function
$$
h(x) \in \{0,\ldots,k-1\}.
$$
In order to handle collisions, we can use a linked list to chain
them together in the bucket.
Searching for an item in the table later involves computing the hash and
then traversing the linked list.

Suppose you put $k$ things in.
You're generally hoping that each bucket tends to have $O(1)$ elements.
In the worst case, you traverse a linked list (and have $O(k)$ lookup).
In the best case, we get $O(1)$ lookup.

What's the runtime of $h$?
Supposing we have $n$ keys, each of which is $10\lg n$ bits.
(Note we're using the RAM  model: the word size of the machine we're
programming is logarithmic in the input size in bits of the problem.
So if we have a million-bit instance, we assume that we can manipulate
words of $\log$ 1m in constant time.)
So that means that our word size is $\Theta(\log n)$ bits.
That is, these keys will fit into a constant number of words.
We'll be relying on this later to assume various operations are $O(1)$.

So for now, we think of $h$ as taking any key and mapping uniformly
random from $0,k-1$.
This is of course not quite right: $h$ must be deterministic.
However, we think of the hash for our purposes as choosing uniformly
at random.

This relates directly to the bin/bucket problems we discussed previously.

\textbf{Claim:}
The average time for a search is $O(1)$, assuming $n$ buckets (with $n$ keys).

No proof (prove this?).
 Basically, the vast majority of buckets will get very few elements.

On the other hand, what's the expected max search time?
That is, has $n$ keys into $n$ buckets with our idealized hash.
The worst-case search time is the longest linked-list we get.
We looked at this last time:
The max load is
$$
\Theta\left(
	\frac{\log n} {\log\log n}
\right)
\,\, w.h.p.
$$
So it's only marginally better than using a red-black tree.

\subsection{Perfect Hashing}

At a high-level, we use \textit{Perfect Hashing} to obviate this
non-constant load problem.
That is the following:

We assume we're dealing with a \textit{static} set of $n$ keys.
That is, we construct a hash-table structure that is specific for the
particular $n$ keys that we have at hand.
As before, assume each key is $10\log_2 n$ bits.

Desiderata:

\begin{itemize}

\item
We want to construct a hash table with $O(1)$ worst case search time.
NOTE this isn't just a matter of expectation: we want guarantees about
the worst case.

\item
We also want to use $O(n)$ space (that is, our solution can't be ``use $O(2^n)$ keys'').

\item
Further, we want ``fast'' construction
\end{itemize}

A naive approach is the following: Repeatedly pick a new hash function
until you find one inducing max load of $O(1)$.
We can do this because we have static keys (we know them in advance).
Eventually we'll find one with $O(1)$ worst case.
However, this won't give us fast construction:
we'll have to run this an exponential number of times in order to 
find this (I don't quite get the proof, but it involves one of the
results we showed last time involving $\log n$).

We'll use a twist on this: we'll follow a similar approach, but our
criterion for selecting a hash function will be looser.
we'll use a two-tiered approach: 
What we'll do is count ``collisions'', and as long as we don't have
more than $n$ collisions, we'll call it good.
So we have $n$ buckets, but instead of a linked list in each bucket,
we have a hash table at each of the $n$ buckets.
That is, we have one primary hash table, and $n$ secondary hash tables.

We want the total size of all these tables to be $O(n)$.
We do our primary hashing; based on how many elements hash into a location,
each bucket has a hash table of that size.
We'll have the size of a secondary hash table being quadratic in the number
of elements there.
So if we have 20 elements in a bucket, we'll have a hash table of size approximately $20^2$.
Why? 
We want to totally avoid collisions at the second level of hashing.
The thing about quadratic size is that this is the threshold at which we'll
suffer 0 collisions in the secondary hash tables (we can repick secondary
hashes).

One concern is that since we're wasting space in the secondary tables,
when we add the size of the secondary tables up, it'll be too large.
So we need to show that the sum of the table sizes will be linear
(intuitively, we'll show the vast majority of buckets get only a constant number
of keys).

So OK getting to details. What is a ``collision''?
Mapping keys to buckets, we'll let $Y_i$ denote the RV corresponding
to the number of keys mapping to bucket $i$ at the top level (for $1\leq i\leq n$).
Let $X$ be the RV denoting the number of collisions.
Then
$$
X = \sum_{i=1}^n {Y_i\choose 2}.
$$
That is, if $10$ keys are mapped to a bucket, that gives 10 choose 2 pairwise
collisions.
Note that
$$
X = \Theta\left(
	\sum_{i=1}^n Y_i^2
\right)
$$
which is, magically, the total size of the secondary hash tables.
So $X$ is constant-factor-related to the space required for the secondary
hash tables.
(Note that it's good enough, then, to pick a hash function that induces
at most $100n$ or $1000n$ collisions.)

Now, to get at $E[X]$, we express $X$ as a sum of indicator variables.
Define $Z_{ij}$ to be the indicator variable taking 1 if the $i$, $j$th keys
collide, and 0 otherwise.
Note there are $n$ choose $2$ such vars.
So
\begin{align}
E[X] 
&=
\sum_{i,j} E[Z_{ij}]  \\
&=
\sum_{i,j} 1/n \\
&=
{n \choose 2} / n
\\
&= 
\frac{n-1}{2}
\end{align}
where we appeal to the fact that we have an idealized hash function.
This is less than $1/2$ of our target of getting $\leq n$ collisions.
(It is important that we choose a constant $> 1/2$ when defining
our criterion for accepting a hash functions.)

Call a hash function ``good'' if it induces $\leq n$ collisions; ``bad'' otherwise.
On average, the number of collisions is $n/2$.
We can therefore bound the percentage of all hash functions that
are ``bad''.
If, for example, 90\% of the has functions are bad, then the expected
number of collisions would be at least $0.9n$.
So we have that at most half of the hash functions are bad.
Note that this is a huge overestimate: the only way
this can happen (that is, we get the expected value we had before)
 is if all the bad hash functions have exactly $n$ collisions and the
 good ones have $0$.

So in our first phrase, picking a primary hash function,
This is, in the worst case, like flipping a fair coin until you get
a heads.
So the expected number of trials is $O(1)$ (it is Geometric(0.5)).
So, with high probability, the number of trials is $O(\log n)$ (proof?).

Once we have the top-level hash function, then, the $Y_i$ values are determined.
In bucket $i$, we use a hash table of size $Y_i^2$.
We repeatedly pick hash functions for bucket $i$ until we find one
that gives no collisions at all.

That is, the secondary hash table at buekct $i$ has $Y_i^2$ size, $Y_i$ keys.
The expected number of collisions is calculated using a similar approach
before.
We define ${Y_i\choose 2}$ indicator variables, and the probability that
one of those is $1$ is $1 / Y_i^2$.
So defining $C_i$ th enum of collisions at $i$, we have
$$
E[C_i] = 
{Y_i\choose 2} (1 / Y_i^2)
=
\frac{Y_i - 1}{2Y_i}
\leq \frac{1}{2}.
$$
The analysis is identical for every bucket.
So cool! We're done, this scheme works.

\subsection{Realistic Hash Functions}

The tricky thing here is that we've been assuming that we have these
idealized has functions that distribute uniformly, and we can generate
them easily (and they're independent).
What is it like in real life?


When we were talking about idealized hash functions, the associated
family corresponds to the family of
$$
n^{n^{10}}
$$
hash functions (this is the number of functions from $10\lg n$ strings to
$n$ things, I think?).
Note if we represent these naively, then we use base-2 log num bits to
specify an element.
This is probematic, because this description of a function is of 
length $n^{10}\lg 10$ bits.

The key thing to note here (that'll get us around this)is that pairwise 
independence is sufficient
for the family of hash functions we use.
What do we mean?
We mean the following:
A family of hash functions $\mathcal H$ is pairwise independent if,
for any distinct keys $x,y$, and any (possibly equal) bucket
indices $i,j$, if the hash function $h$ is drawn uniformly at random
from $\mathcal H$,
then
$$
Pr(h(x) = i\,\,\&\,\, h(y) = j)
=
\frac{1}{B}
$$
with $B$ the number of buckets.

This is sufficient because, in our analysis, we were only interested in
pairwise indicator variables. 
We were never interested in any more complicated conditions.



We're interested in designing a family of functions mapping from
$(10 \log_2 n)$-bit strings to $(\log_2 n)$-bit strings.

We saw taht for analysis, we didn't need full independence, but just
pairwise independence.
This is because our analysis cared only about the number of pairwise collisions,
which we can write as a sum of indicator variables $X_{ij}$, which is
1 iff $i$ and $j$ collide.
Recall that $P\{X_{ij} = 1\} = 1/p$, with $p$ the number of buckets.

In our analysis, in fact, it's fine to have approximate pairwise independence;
that can give us OK bounds.
So we want a function $h$ such that, for two distinct keys $x\neq y$ (and any $i,j$, not necessarily distinct),
then
$$
Pr\{
	h(x) = i \,\wedge\,
	h(y) = j
\}
=
O\left(
	\frac{1}{n^2}
\right).
$$
IN the above, $i$ and $j$ are bins; $x$ and $y$ are keys.

First, we'll pick a prime $p$ a bit bigger than $n^{10}$ (why?).
The prime number theorem tells us that about, picking things around the
neighborhood, we only have to try a logarithmic number of keys (in $n^{10}$)
in expectation before finding a prime.
(There is also a theorem indicating that there is definitely a prime
between $k$ and $2k$; in particular, this is $n^{10}$ and $2n^{10}$).

So let's hash from $\Z_p$ to $\Z_n$.
Consider
$$
h(x) = \left[ ax + b \mod p\right]
\mod n
$$
with $a$ and $b$ chosen from $\Z_p$.
(Note that we have $a$ and $b$ because we want it to be the case that,
under our analysis, there will be no ``bad input''---if we have an adversary
picking keys, they won't be able to pick bad keys if they know what the
family looks like; if there were no $a$ and $b$, then they would be
able to do so. In other words, our family needs to have more than one
hash function in it if we want to argue a small number of expected
collisions.)
This isn't quite uniform between $0$ and $n-1$, but we're quite close.
The inner hash
$$
ax + b \mod p
$$
is, I think, actually uniform over $\Z_p$ (proof?).
The outer hash, then, will be approximately uniform over $\Z_m$ (proof?).
This satisfies the condition for approximate pairwise independence
that we defined earlier.

So at the top level, we pick a $p$, then we repeatedly choose $a$ and $b$; for each
$a$ and $b$, we check how the particular hash function works.
We repeatedly do that until we get a good $a$ and $b$; we store them,
as they fully parametrize a hash function.
We do this for each primary hash function and each secondary hash function.





\section{1/29/2013: Dynamic Programming}

\subsection{Examples}

\subsubsection{contiguous subarrays of booleans}
Suppose we're given an $n\times n$ boolean array $A$.
We want to find the side-length of a largest all-true contiguous square
subarray of $A$.
So if there's a $3\times 3$ subarray of Trues, then the desired answer
is at least 3.

Now, certainly this problem is solvable in polynomial time.
For each side length $k$, there are only $n^2$ different places where the
$k\times k$ subarrays top-left corner could be; we could
just exhaustively check this for all $k$ and compute the answer.

We can get a better polynomial bound using Dynamic Programming.
What DP does is, instead of solving a single problem instance, find a bunch
of problem instances to solve, and do so in such a way that the larger
problem instances can use the computations performed for the smaller
instances.

So let $a_{ij}$ be the size of the largest all-true contiguous
square subarray with lower right corner
at location $(i,j)$.
This is a family of $n^2$ subproblems.
Our answer to the original problem is just
$$
\max_{i,j} a_{ij}.
$$

Now, what is a good order to solve these $n^2$ subproblems?
What we'd like is that, whenever we get to a particular problem
in this ordering, we can very easily solve it in terms of the instances
we've already solved.
In other words, we want to be able to write a suitable reccurrence for
the $a_{ij}$'s.
Thinking about it, it's not too tough to see the following works:
\begin{equation}
a_{ij} = 
\left\{
\begin{array}{ll}
0 & \textrm{if } (i,j) \textrm{ entry is F}
\\
1 + \min(a_{i-1,j}, a_{i,j-1}, a_{i-1,j-1}) & \textrm{otherwise}
\end{array}
\right.
\end{equation}
So filling in left-to-right row by row (or top-down column by column) should
work.
The important thing is that, when we get to a position, 
the spot above it, to the left of it, above and to the left of it, are
filled in.
(A small note is that we need to interpret out-of-bounds indices as 0 above.)

So the above algorithm runs in $O(n^2)$ time, which is a nice,
much-faster polynomial-time algorithm than the brute-force poly-time algorithm.

\subsubsection{Rod-cutting problem}

Commonly, Dynamic programming yields polynomial-time algorithms for
problems where the brute-force technique is exponential.
For example, suppose we have a rod of length $n$ inches ($n\in \Z^+$).
We can cut it into any integer-length pieces (such that they sum to $n$).
For every $i$, we have a price $p_i$ that we can sell a rod of length $i$
for.
The problem, then, is to cut up the rod into pieces in such a way that
we maximize the total prices for the pieces.

When we cut the number up, we get a multiset of positive integers; in
the language of number theory, this is a \emph{partition} of $n$.
If the number of partitions of $n$ were sufficiently slow, we could just
generate them and calculate their value; however, the number of partitions
grows superpolynomially.
Consider the numbers
$$
1,2,\ldots, \lceil 2\sqrt n \rceil - 1,
\lceil 2\sqrt n \rceil.
$$
Pair up the first two numbers and the last two numbers:
$$
1,2,\ldots, \lceil 2\sqrt n \rceil - 1,
\lceil 2\sqrt n \rceil.
$$
For the first $\lceil 2\sqrt n\rceil + 1$ elements, you can either
partition them into the two outermost pieces or the next
innermost pieces.

We repeat for $3,4$ and the next two inner numbers on the right.
For each of these, we eat up $\sqrt n$ of the rod.
We'll have $\sqrt n / 2$ such decisions for what to do with the
pieces of length $\sqrt n$, and we'll have about
$$
2^{\Omega(\sqrt n)}
$$
ways to partition the rods up.
(In fact, it ends up being $2^{O(\sqrt n)}$ too.)
So the number of partitions of $n$ is pretty clearly superexponential.

We can solve this, however, with a simple one-dimensional dynamic program.
For each $i$, let $v_i$ be the maximum value of a rod of
length $i$.
We will use the following recurrence for $v_i$ in terms of the smaller $v_j$'s.
For a partition of $i$, there will be some length $\ell$ of the last
partition.
So
\begin{align*}
v_0 &= 0 \\
v_i &=
\max_{1\leq \ell \leq i}
v_{i-\ell} + p_\ell
\end{align*}
We get $O(n^2)$ time of this: we have $n$ subproblems, but the $i$th
subproblem takes $O(i)$ time to solve (summing over these subproblems
uses the familiar $\sum_i i^2 = O(n^2)$ identity).


\subsubsection{Longest Common Subsequence}

We have two sequences $X,Y$ of symbols over some finite alphabet.
Writing $X$ as
$$
X = x_1 x_2 \dots x_n,
$$
we define a subsequence of $X$ as a subset of entries $x_i$, concatenated
in order.
We want to find the largest integer $k$ such that there are
subsequences in $X$ and $Y$ of length $k$ such that both subsequences are
the same.

The brute-force approach here is exponential: it's pretty easy to see (there are $2^n$ subsets of $n$ sets).

We use the familiar two-dimensional DP solution.
Let $X_i$ denote the length-$i$ prefix of $X$:
$$
X_i = x_1\dots x_i,
$$
and similarly with $Y_j$.
Now, let
$a_{ij}$ be the length of the LCS of $X_i$ and $Y_j$.

Supposing $|X| = m$ and $|Y| = n$, we have $m\times n$ subproblems.
The final answer to the question is $a_{mn}$.
So we have the base cases
\begin{align*}
a_{0j} &= 0 \\
a_{i,0} &= 0
\end{align*}
for all $i,j$.
For the recursive case, we have
\begin{equation}
a_{ij} =
\left\{
	\begin{array}{ll}
		a_{i-1,j-1} + 1 & \mathrm{if } x_i = y_j \\
		\max\{a_{i,j-1}, a_{i-1,j}\} & \mathrm{if } x_i \neq y_j.
	\end{array}
\right.
\end{equation}




\subsubsection{The Partition Problem (a pseudopolynomial algorithm)}

This problem is NP-hard.
Suppose we have $n$ positive integers $x_1, \ldots, x_n$.
We want to know whether the $x_i$'s can be partitioned into two multisets
of equal sum.

We give a DP algorithm.
First, let $\sum x_i = 2s$.
(Note if the sum is odd, we return that there is no such partition).
To determine $s$, it' a natural thing to consider computing
\beq
a_{i,j} = \left\{
\begin{array}{ll}
	T & \textrm{if there's a subseq. of $x_1,\ldots,x_i$ summing to exactly $j$} \\
	F & o.w.
\end{array}
\right.
\eeq
Note $\forall i$, $a_{i,0} = T$.
Further, $\forall j > 0$ $a_{0,j} = F$.
When $i,j > 0$, we have
\beq
a_{i,j} = \left\{
\begin{array}{ll}
	a_{i-1,j} & 
	\textrm{if } j < x_i
	\\
	a_{i-1,j} \vee a_{i-1,j-x_i}
	& o.w.
\end{array}
\right.
\eeq
NB we only really need the first term for the technicality of the subscript
on the RHS disjunct being negative.

Now, the number of table entries is $O(nS)$.
Is this polynomial time?
Intuitively, no---$S$ can be very large.
We argue that the algorithm is not polynomial; to do so, we need to find
one family of inputs where the runtime is not upper-bounded by a polynomial
function of the input in bits.

What do we mean by polynomial time?
Well, we upper-bound the runtime of an algorithm according to a polynomial
function of its input in bits.
What's the input of this like?
Well, consider an input where each $x_i$ is an $n$-bit integer.
The input size for this problem is $\Theta(n^2)$ bits (we have $n$ $n$-bit numbers).
For us to claim that we have a polynomial runtime, we need that, for such
inputs, the runtime is polynomial in $n$.
$S$, however, is $O(n 2^n)$, since an $n$-bit integer can be as large
as $2^n$ in value (and we have $n$ of them).
So, more importantly, $S$ can be $\Omega(n 2^n)$.
So this algorithm, which kinda looks like it's polynomial, actually
isn't polynomial in the input size.

However, sometimes these sorts of algorithms are useful.
If we know, for example, that each of the numbers is at most $n^3$, then we have
that $S$ is at most $n^4$, and we have a polytime algorithm.
We call algorithms like this \emph{pseudopolynomial}: they are
polynomial in the input size if the input integers are represented
in unary.


\section{1/31/2013: Greedy Algorithms}

\subsection{A Scheduling Problem}

Suppose we have $n$ tasks, each with a positive integer deadline and an
execution requirement.
So an input may look like

\begin{tabular}{| lll |}
\hline
task & Deadline & execution requirement \\
A & 8 & 5 \\
B & 6 & 2 \\
C & 9 & 2 \\\hline
\end{tabular}

We want to know if we can meet all the deadlines in a nonpreemptive schedule?
So, in the above, suppose we run $B$ at time 0 and $C$ at time 2; both
of these take time 2, and have met their deadlines. 
If we run $A$ next, though, it'll miss its deadline.
However, if we run in the order $BAC$, we get a feasible schedule.

We may want to try to get at this by Dynamic Programming, but we don't
need it: a greedy solution ends up sufficing.
We use \emph{earliest deadline}, breaking ties arbitrarily.
This will yield a feasible schedule if one exists.

We want to argue that the answer to the greedy algorithm is right iff there's
a feasible schedule.
Clearly, Left to Right is easy.
The argument is R to L, then: If there's a feasible schedule, then we need
to show the greedy algorithm finds it.

So take a feasible schedule.
Assume the feasible schedule has no ``spaces'': there's no advantage
to idle time, so everything is ``squeezed'' as far as possible to the left.
Suppose that we have a schedule $ABCD$ of four elements, in that order.
Suppose that $C$ has an earlier deadline than $B$; earliest deadline
would have chosen the order $ACBD$, rather than $ABCD$.

We can modify the schedule by leaving everything the same except for 
$B$ and $C$, swapping the latter two.
Note the new schedule is clearly the same length.
We want to show that $ACBD$ is also feasible.
Clearly $C$ meets it deadline still---it's moved earlier.
We need to show that $B$ is meeting its deadline.
Since, by assumption, $C$ had a more stringent deadline than $B$, it must
be the case that if $B$ finishes when $C$ did before, it certainly meets
its deadline, since it meets $C$'s deadline.
In other words, before, with $ABCD$, $C$ was meeting its deadline; we know that 
$B$'s deadline is later than $C$'s, so in $ACBD$, $B$ definitely meets its deadline.

We apply the same logic to the new problem instance to argue that it's feasible 
(eventually we will get to the earliest-first schedule).




\subsection{A variation of the scheduling problem}

Suppose that, in addition to the deadline and execution requirements,
each task has a positive profit $p_i > 0$ associated with it.
Our goal is to find a max-profit feasible subset (that is, we won't in general
use the entire set of tasks).
What we'll do is combine the greedy approach with dynamic programming.

We first order tasks by deadline (in nondecreasing order).
We then consider all prefixes with respect to that particular ordering.
The DP's subproblem is $a_{k,t}$, defined as the max profit we can
obtain with a schedule using only the first $k$ tasks and time $\leq t$.
We can write a recurrence fairly straightforwardly (left as an exercise).

The runtime of the above will be polynomial if the execution requirements
are polynomially-bounded.
That is, if every task has a task that is $O(n^c)$ for some $c$.
Without such an assumption, we could get an exponentially-sized
table in our DP


\subsection{Matroids}

Captures a large class of greedy algorithms.
Consider the MInimal Spanning Tree problem, in particular, Kruskal's
greedy algorithm.
One way to convince ourselves that Kruskal's algorithm is correct
is to reason directly about minimal spanning trees.
Another is to frame the algorithm as a matroid problem, and use a general
property about Matroids.

A Matroid is a pair
$$
M = (S,\mathcal I)
$$
with $S$ analogous to the vertices in a graph: it's just a set.
$\mathcal I$ is called the \emph{independent sets} of $S$: $\mathcal I \subseteq 2^S$ (elements are subsets of $S$).
We need two more properties:
\begin{enumerate}
	\item
	\textbf{Hereditary Property}: if $A\in\mathcal I$ and $B\subseteq A$, 
	then $B\in\mathcal I$.
	
	\item
	\textbf{Exchange Property}:
	If $A,B\in \mathcal I$ and $|A| > |B|$, then
	there is some $x \in A\setminus B$ such that $B\cup\{x\} \in \mathcal I$.
\end{enumerate}
The hereditary property tells us that removing elements from an independent
set yields an independent set.
In particular, this tells us that $\emptyset\in\mathcal I$.

Define a maximal independent set as an independent set such that, if you
add any additional elements to it, it will no longer be independent.
The exchange property tells us that all maximal independent sets
will have the same cardinality.

In a \textbf{weighted matroid}, each element $x\in S$ has an associated
weight $w(x)$.
In applications, we often want to compute a maximum (or minimum) weight
maximal independent set.
For example, eventually we'll have a minimal spanning tree representing a
minimum weight maximal independent set (the maximal independent sets will
correspond to the spanning trees).




\section{Matroid Greedy Algorithm}

Suppose we want to do maximization.
\begin{itemize}

\item
First, sort the elements of $S$ in nonincreasing order of weight (if we
are minimizing, we'll sort in the other direction).

\item
Initialize
$
A := \emptyset
$

\item
For each $x\in S$, in the order determined in the first step:
	\begin{itemize}
		\item
		If $A\cup \{x\} \in \mathcal I$, then $A := A \cup \{x\}$.
	\end{itemize}
\end{itemize}
And that's it.
Thinking about Kruskal's algorithm, the set $\mathcal I$ will be the set
of acyclic graphs---the conditional above corresponds to the conditional
in that algorithm.


How do we show this algorithm is correct?
First, index the elements of $S$ $1,\ldots,n$ according to the
ordering in the first step.
Call the weight for element $i$ in this ordering $w_i$.
For each $i$, calculate some max-weight maximial independent set $B$.
Mark down if element $i$ is in $B$ or not.
Also, for each $i$, consider $A$, the set computed at that step by the algorithm.
What we need to do is prove, for each step, $A$ is a maximal weight independent step.
Similarly, look at whether elem $i$ is put in $A$.

Look at the first part where the sets $A$ and $B$ differ.
It can't be the case that the elem $i$ is in $B$ but not in $A$ (proof? It has
somethign to do with the hereditary property, not sure what).
So the first place where the two differ is that an element $i$ must
be in $A$ and not in $B$.
WE note the first place $i$ where $A$ and $B$ differ.
Construct $A'\subset A$ from all the before $i$ in the ordering, and also elem $i$.
Now, we know $|B| \geq A'$, since $|A'| \leq |A|$, and $|B| = |A|$, by the
exchange property.

We repeatedly apply the exchange property to add elements from $B$ to $A'$.
The exchange property tells us that we can add an element from $B$ to $A'$
and get an independent set, so long as $|A'| < |B|$.
This process terminates exactly when $|A'| = |B|$.

When we're done growing $A'$, $B$ gave $A'$ all its elements from $i+1$ onwards
except for one of them.
In other words,
$$
A' = (B + i) = j
$$
for some $j$.
That is $A'$ has elem $i$ while $B$ doesn't, and $A'$ deosn't get some
$j>i$.
However, since the elements are ordered by weight, and $w_j \leq w_i$,
we can conclude that
$$
w(A') \geq w(B).
$$

Now we play the whole game over, but with $B$ replaced by $A'$.
This gets us ``one step'' in the right direction, and we iterate
this process until we end up with $A' = A$, at which point we can
argue that $A$ was a max-weight maximal independent set.
 

























\end{document}